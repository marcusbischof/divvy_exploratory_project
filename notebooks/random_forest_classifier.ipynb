{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "#\n",
    "# Marcus Bischof\n",
    "#\n",
    "# Random Forest Classifier to Predict the likelihood that a given trip, originating\n",
    "# from a given station, will end in a different neighborhood.\n",
    "# \n",
    "# Why did I choose to predict this particular thing?\n",
    "#\n",
    "#    1) I imagine that a high priority for Divvy is to ultimately predict the capacity of racks & bikes\n",
    "#       that should be stocked at each station.\n",
    "#         1a) This is a difficult task, that may require significant effort to predict\n",
    "#              1aa) I believe that more features (than provided..) would be required to accurately predict\n",
    "#                   capacity. The divvy data that I have only provides the total\n",
    "#                   capacity of a station as opposed to the total # of FREE racks.\n",
    "#\n",
    "#    2) I would like to contribute to predicting capacity, as I imagine an accurate prediction would likely\n",
    "#       require an ensemble of models.\n",
    "#         2a) THUS, I will build a model that will predict the liklihood that a trip ends in a different\n",
    "#             neighborhood vs. the same neighborhood.\n",
    "#              2aa) I will use a random forest classifier to do this, and I will use the neighborhood\n",
    "#                   data that I acquired from the City of Chicago (https://data.cityofchicago.org/) to label\n",
    "#                   the two classes.\n",
    "#              2ab) This should benefit a larger model, because if this model predicts that a trip will\n",
    "#                   end in another neighborhood with a high likelihood, one can assume that the meta-model\n",
    "#                   would slightly increase the amount of future capacity at the station that this trip\n",
    "#                   originated from.\n",
    "#\n",
    "########################################################################################\n",
    "\n",
    "# The basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Viz\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Jupyter display\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "df = pd.read_pickle('../data/interim/df_0_1000000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that we wouldn't have at the start of a trip\n",
    "for feature in [\n",
    "    'stoptime', 'to_station_name', 'to_station_id', 'latitude_end', 'longitude_end',\n",
    "    'dpcapacity_end', 'to_neighborhood', 'tripduration', 'same_station_trip'\n",
    "]:\n",
    "    del df[feature]\n",
    "\n",
    "# Remove extraneous and/or non-predictive features (day, month, year, hour cover starttime)\n",
    "for feature in [\n",
    "    'trip_id', 'from_station_id', 'starttime'  \n",
    "]:\n",
    "    del df[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.same_neighborhood_trip = df.same_neighborhood_trip.apply(lambda x : int(x))\n",
    "df.from_neighborhood = df.from_neighborhood.astype('category')\n",
    "\n",
    "# Change categorical columns to numerics\n",
    "categoricals = [col for col in df.columns if df[col].dtype.name == 'category']\n",
    "for categorical_column in categoricals:\n",
    "    df[categorical_column] = df[categorical_column].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that we are trying to predict: 1 -> same neighborhood trip, 2 -> different neighborhood trip\n",
    "df.same_neighborhood_trip.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#\n",
    "# Deal with class imbalance, create train + test datasets\n",
    "#\n",
    "########################################################\n",
    "\n",
    "# OH JOY. We have a class imbalance. Let's resample.\n",
    "df_resample = pd.DataFrame()\n",
    "df_same_hood = df[df.same_neighborhood_trip == 1]\n",
    "df_diff_hood = df[df.same_neighborhood_trip == 0]\n",
    "\n",
    "# Get length of same hood frame, we downsample df_diff_hood to this smaller length\n",
    "num_same_hood = len(df_same_hood)\n",
    "\n",
    "df_resample = resample(df_diff_hood, replace=False, n_samples=num_same_hood, random_state=0)\n",
    "\n",
    "# Concatenate the frames\n",
    "df_resample = pd.concat([df_resample, df_same_hood])\n",
    "\n",
    "target = df_resample.pop('same_neighborhood_trip')\n",
    "features = df_resample\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#\n",
    "# Random forest classifier.\n",
    "# Cross Validation used to avoid overfitting.\n",
    "# GridSearchCV used to tune hyper-parameters (yes, I've done minimal tuning here).\n",
    "#\n",
    "########################################################\n",
    "\n",
    "rf_clf = RandomForestClassifier(class_weight='balanced',verbose=0,random_state=10,n_jobs=4)# instantiate model\n",
    "\n",
    "n_estimators = np.linspace(10, 300, 6, dtype='int32')  # used to determine the number of estimators\n",
    "max_features = [5, 7, 9, 11, 14]  # used to determine max number of features, we have a max of 14, but I would like 5 at the minimum\n",
    "\n",
    "param_grid = dict(n_estimators=n_estimators, max_features=max_features, max_depth=[1,50])  # set up the grid for GridSearchCV\n",
    "clf = GridSearchCV(rf_clf, param_grid=param_grid, cv=3, scoring='recall_macro')  # instantiate cross validation instance\n",
    "\n",
    "clf.fit(X_train, y_train)  # train the classifier\n",
    "\n",
    "# Test model\n",
    "y_true, y_pred = y_test, clf.predict_proba(X_test)  # probability prediction for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, clf.predict(X_test))  # create performance report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#\n",
    "# Display top 15 features importances.\n",
    "# Usefull when we have enough features to obscure view on bar chart.\n",
    "#\n",
    "########################################################\n",
    "\n",
    "# Create plot of feature importance\n",
    "feature_names = X_train.columns.values.tolist()\n",
    "important_features = clf.best_estimator_.feature_importances_  # Important features\n",
    "\n",
    "indices = np.argsort(important_features)\n",
    "feature_names_ordered = [feature_names[i] for i in indices]\n",
    "\n",
    "fig, ax1 = plt.subplots(1, 1)\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), important_features[indices], color='g', align='center')\n",
    "plt.yticks(range(len(indices)), feature_names_ordered)\n",
    "ax1.set_xticklabels\n",
    "plt.xlabel('Relative Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  <b><font color='green'>Possibilities of overfitting</font></b>\n",
    "   -  <b><font color='green'>hour + week + day --> temperature</font></b> (hour week and day <i>partly encode</i> temperature since various weeks are highly correlated with temperature). However, hour, week, and day are also significant because they determine things like: is it rush hour, is this a vacation week, is this a weekday vs. weekend, ...?\n",
    "   -  <b><font color='green'>from_neighborhood --> latitude_start + longitude_start</font></b> (from_neighborhood only adds a little bit of extra pizzaz to latitude_start and longitude_end). It is obvious that a neighborhood is a polyline encircling (lat, long) tuples on a map, so in a way it is a more abstracted geocoded object. HOWEVER, as stated earlier a neighborhood has a vibe, spirit, and unique identity - it has a community. Residents gravitate towards neighborhoods in many cases due to these neighborhood identities. Some (lat, long) pairs may be close together, but residents biking from stations that are close but in different neighborhoods may treat bike trips differently. Thus, we will keep lat, long, AND from_neighborhood. HOWEVER, <b><font color='red'>from_station_name will be removed from the future model</font></b> since (lat, long) represents the orginating station.\n",
    "\n",
    "\n",
    "Naturally, temperature is highly correlated with trip duration, and with the amount that humans sweat... It makes sense that this would help narrow down the liklihood that a long trip to another neighborhood will or will not occur. If it's february and freezing, I'm sure we are seeing fewer long inter-neighborhood trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#\n",
    "# Standard deviation for the test score (precision) reported.\n",
    "#\n",
    "########################################################\n",
    "\n",
    "print(clf.cv_results_['std_test_score'][list(clf.cv_results_['rank_test_score']).index(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our model to a pickle, fitting this bad boy was intense.\n",
    "filename = 'neighborhood_model_fitted_to_df_0_1000000.sav'\n",
    "pickle.dump(rf_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
